{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "- Dimensionality reduction refers to reducing the number of input variables for a dataset. \n",
    "- The resulting dataset, the projection, can then be used as input to train a ML model.\n",
    "- A popular approach to dimensionality reduction is to use techniques from the field of linear algebra. This is often called feature projection and the algorithms used are referred to as projection methods.\n",
    "\n",
    "### Load  iris dataset\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "print('Iris dataset')\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('\\tshapes:', X.shape, y.shape)\n",
    "        \n",
    "target_names = iris.target_names\n",
    "print('\\ttarget_names:', target_names)\n",
    "# print(iris.data)\n",
    "\n",
    "def plot_reduced_dataset(X_r, fig_title):\n",
    "    plt.figure()\n",
    "    colors = ['navy', 'turquoise', 'darkorange']\n",
    "    lw = 2\n",
    "\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "        plt.scatter(X_r[y == i, 0], \n",
    "                    X_r[y == i, 1], \n",
    "                    color=color, \n",
    "                    alpha=.8, \n",
    "                    lw=lw,\n",
    "                    label=target_name)\n",
    "    \n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title(fig_title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Factorization\n",
    "\n",
    "Matrix factorization methods can be used to reduce a dataset matrix into its constituent parts, e.g., by:\n",
    "- EigenDecomposition\n",
    "- **Singular Value Decomposition (SVD)**\n",
    "\n",
    "These constituent parts can be ranked and then selected that best captures the salient structure of the matrix, e.g., by **Principal Components Analysis (PCA)**\n",
    "\n",
    "There is no best technique for dimensionality reduction and no mapping of techniques.\n",
    "\n",
    "\n",
    "## 2.1 (Truncated) Singular Value Decomposition (SVD)\n",
    "Truncated SVD works on term count *tf-idf* matrices as returned by the vectorizers in *sklearn.feature_extraction.text*. In that context, it is known as **latent semantic analysis (LSA)**.\n",
    "This estimator supports two algorithms: a fast randomized SVD solver as an eigensolver on $X * X.T$ or $X.T * X$, which ever is more efficient.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "print('SVD: reduced shape', X_svd.shape)\n",
    "print('SVD: explained variance ratio (first two components): %s'\n",
    "      % str(svd.explained_variance_ratio_))\n",
    "\n",
    "# print(X_svd)\n",
    "plot_reduced_dataset(X_svd, 'SVD of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Principal Component Analysis (PCA)\n",
    "Linear dimensionality reduction using Singular Value Decomposition (SVD) of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. PCA identifies the combination of attributes that account for the most variance in the data.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print('PCA: reduced shape', X_pca.shape)\n",
    "print('PCA: explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plot_reduced_dataset(X_pca, 'PCA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Discriminant Analysis (LDA)\n",
    "LDA is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. LDA tries to identify attributes that account for the most variance between classes. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)\n",
    "print('LDA: reduced shape', X_lda.shape)\n",
    "\n",
    "plot_reduced_dataset(X_lda, 'LDA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Manifold Learning\n",
    "\n",
    "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.\n",
    "\n",
    "**Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data.**\n",
    "\n",
    "Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.\n",
    "\n",
    "\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/manifold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 MultiDimensional Scaling (MDS)\n",
    "\n",
    "MDS seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "In general, MDS is a technique used __for analyzing similarity or dissimilarity data__. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "embedding = MDS(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X[:100])\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Isomap Embedding\n",
    "\n",
    "Isometric Mapping (Isomap) seeks a lower-dimensional embedding which maintains geodesic distances between all points. \n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/manifold.html#isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "embedding = Isomap(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X[:100])\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE (TSNE) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student’s t-distributions. \n",
    "This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:\n",
    "- Revealing the structure at many scales on a single map\n",
    "- Revealing data that lie in multiple, different, manifolds or clusters\n",
    "- Reducing the tendency to crowd points together at the center\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "print(X_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
